# MacHand
MacHand: Large-scale video dataset for cross-primate hand tracking.

<p align="center">

## <strong><a href="URL_FOR_PAPER">Paper</a></strong> | <strong><a href="URL_FOR_PROJECT_PAGE">Project Page</a></strong> | <strong><a href="URL_FOR_HUGGINGFACE_DEMO">Demo</a></strong>

</p>

## Abstract

Primate hands serve as essential tools in both social communication and dexterity-related tasks. Harnessing the potential of deep learning, we can meticulously track these hand movements, paving the way for high-throughput, video-based movement quantification. However, there exists a discernible gap in datasets and models designed for non-human primate hand tracking. Here, we present MacHand, a novel cross-primate species dataset comprising over 10,000 videos of primates during social interactions and common tool manipulation tasks with hand bounding box and hand landmark annotations. To facilitate primate hand detection and landmark quantification, we present state-of-the-art models based on both transformer and light-weight architectures, demonstrating competitive performance and generalizability across primate species in hand detection and hand landmark tracking. Finally, we contribute MacHand-Anno, our browser-based hand pose annotation tool for light-weight and efficient hand landmark annotation. By combining machine learning and primatology, MacHand can enable novel research exploring the intricate lives of primates, yielding insights for monitoring primate populations, informing conservation efforts, and deepening our understanding of primate social behavior and cognition.

## Demo
